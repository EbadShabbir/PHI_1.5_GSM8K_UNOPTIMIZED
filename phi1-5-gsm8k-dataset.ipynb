{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nimport torch, time, psutil, numpy as np, gc, re, json\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge_score import rouge_scorer\nimport evaluate\n\n# Load model and tokenizer\nmodel_name = \"microsoft/phi-1_5\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.padding_side = \"left\"\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, trust_remote_code=True,\n    torch_dtype=torch.float16, device_map=\"auto\"\n)\ndevice = model.device\ntorch.cuda.empty_cache()\n\n# Load dataset\ndataset = load_dataset(\"gsm8k\", \"main\", split=\"test[:500]\")\n\n# BLEU smoothing\nsmoother = SmoothingFunction().method4\n\ndef extract_answer(text):\n    match = re.search(r\"(?:Answer:)?\\s*([-]?\\d*\\.?\\d+|\\d+/\\d+|[-]?\\d+|\\d+\\s*\\w+)\", text)\n    return match.group(1) if match else text.strip()\n\ndef evaluate_phi15_on_gsm8k(dataset, batch_size=8, max_new_tokens=50):\n    accuracy_metric = evaluate.load(\"accuracy\")\n    f1_metric = evaluate.load(\"f1\")\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n    process = psutil.Process()\n\n    results = {k: [] for k in [\n        \"accuracies\", \"f1_scores\", \"latencies\", \"tokens_per_sec\", \"memory_usage\",\n        \"perplexities\", \"bleu_scores\", \"rouge1_scores\", \"rougeL_scores\",\n        \"retrieval_latencies\", \"memory_reductions\", \"query_times\",\n        \"accuracy_drops\", \"compression_ratios\", \"knowledge_retentions\"\n    ]}\n\n    initial_memory = process.memory_info().rss / 1024**3\n\n    for i in range(0, len(dataset), batch_size):\n        batch = dataset[i:i+batch_size]\n        prompts = [f\"Solve: {q}\\nAnswer: \" for q in batch[\"question\"]]\n        references = [str(a).split(\"#### \")[-1].strip() for a in batch[\"answer\"]]\n        ref_explanations = [str(a).split(\"#### \")[0].strip() for a in batch[\"answer\"]]\n\n        start_retrieval = time.time()\n        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n        retrieval_latency = time.time() - start_retrieval\n        results[\"retrieval_latencies\"].append(retrieval_latency)\n\n        start = time.time()\n        with torch.no_grad():\n            outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n        latency = time.time() - start\n        results[\"latencies\"].append(latency)\n\n        generated_tokens = sum(len(out) - len(inp) for out, inp in zip(outputs, inputs['input_ids']))\n        results[\"tokens_per_sec\"].append(generated_tokens / latency if latency > 0 else 0)\n        results[\"query_times\"].append(time.time() - start)\n\n        final_memory = process.memory_info().rss / 1024**3\n        results[\"memory_usage\"].append(final_memory)\n        results[\"memory_reductions\"].append(max(0, initial_memory - final_memory))\n\n        generated_texts = [tokenizer.decode(out, skip_special_tokens=True) for out in outputs]\n        pred_answers = [extract_answer(text) for text in generated_texts]\n\n        try:\n            accuracy = accuracy_metric.compute(predictions=pred_answers, references=references)[\"accuracy\"]\n            f1 = f1_metric.compute(predictions=pred_answers, references=references, average=\"macro\")[\"f1\"]\n        except:\n            accuracy, f1 = 0, 0\n        results[\"accuracies\"].append(accuracy)\n        results[\"f1_scores\"].append(f1)\n\n        for gen, ref_exp, ref_ans in zip(generated_texts, ref_explanations, references):\n            gen_words = gen.split()\n            ref_words = ref_exp.split() if ref_exp.strip() else ref_ans.split()\n\n            try:\n                bleu = sentence_bleu([ref_words], gen_words, weights=(0.5, 0.5, 0.0, 0.0), smoothing_function=smoother)\n                results[\"bleu_scores\"].append(bleu)\n            except:\n                results[\"bleu_scores\"].append(0)\n\n            try:\n                rouge = scorer.score(ref_exp if ref_exp.strip() else ref_ans, gen)\n                results[\"rouge1_scores\"].append(rouge['rouge1'].fmeasure)\n                results[\"rougeL_scores\"].append(rouge['rougeL'].fmeasure)\n                results[\"knowledge_retentions\"].append(rouge['rougeL'].fmeasure)\n                results[\"accuracy_drops\"].append(1 - rouge['rouge1'].fmeasure)\n            except:\n                results[\"rouge1_scores\"].append(0)\n                results[\"rougeL_scores\"].append(0)\n                results[\"knowledge_retentions\"].append(0)\n                results[\"accuracy_drops\"].append(0)\n\n            try:\n                input_tokens = len(inputs['input_ids'][0])\n                output_tokens = len(outputs[0])\n                results[\"compression_ratios\"].append(input_tokens / output_tokens if output_tokens > 0 else 1)\n            except:\n                results[\"compression_ratios\"].append(1)\n\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    # Final metrics\n    summary = {k: np.mean(v) if v else 0 for k, v in results.items()}\n    for k, v in summary.items():\n        print(f\"{k.replace('_', ' ').title()}: {v:.3f}\")\n\n    with open(\"/kaggle/working/gsm8k_phi15_results.json\", \"w\") as f:\n        json.dump(summary, f, indent=2)\n\n    return summary\n\n# Run evaluation\ngsm8k_metrics = evaluate_phi15_on_gsm8k(dataset, batch_size=8, max_new_tokens=50)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-04-18T17:34:26.808450Z","iopub.execute_input":"2025-04-18T17:34:26.808754Z","iopub.status.idle":"2025-04-18T17:36:49.297849Z","shell.execute_reply.started":"2025-04-18T17:34:26.808732Z","shell.execute_reply":"2025-04-18T17:36:49.296979Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Accuracies: 0.012\nF1 Scores: 0.007\nLatencies: 1.755\nTokens Per Sec: 226.146\nMemory Usage: 1.826\nPerplexities: 0.000\nBleu Scores: 0.161\nRouge1 Scores: 0.397\nRougel Scores: 0.259\nRetrieval Latencies: 0.002\nMemory Reductions: 0.000\nQuery Times: 1.755\nAccuracy Drops: 0.603\nCompression Ratios: 0.651\nKnowledge Retentions: 0.259\n","output_type":"stream"}],"execution_count":13}]}